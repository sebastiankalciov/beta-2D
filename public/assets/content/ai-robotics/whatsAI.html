<html>
    <head>
        <link rel="stylesheet" href="styles.css">
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </head>

    <body>
    <div class="content">
        <h1>What is Artificial Intelligence (A.I.)?</h1>
            <p>The field of Artificial Intelligence concerns itself with building machines that are able to compute how to act effectively in situations where traditional algorithmic approaches fail and emulating human-like intelligence is better suited.</p>
            <p>Naturally, this raises the problem of defining intelligence: some researchers view it in terms of fidelity to human performance, while others gave it the more formal definition of rationality - taking the "right decisions" to arrive at its goal in a more mathematical manner, so to say.</p>
            <p>Through the lens of the former, Alan Turing proposed in the year of 1950 the following thought experiment: A computer passes the Turing test if a human interrogator, after posing a series of written questions, is not able to tell whether the responses they get come from a person or from a computer.</p>
            <p>In order for a computer to pass a rigorously applied such test, it would need to possess the following capabilities:</p>
            <ul>
                <li><b>Natural language processing</b> - to be able to communicate in a human language,</li>
                <li><b>Knowledge representation</b> - to store the information that it learns,</li>
                <li><b>Automated reasoning</b> - to answer questions and generate new conclusions,</li>
                <li><b>Machine learning</b> - to adapt to newly encountered circumstances and to draw out patterns.</li>
            </ul>
            <p>Furthermore, while the original Turing test did not view the physical simulation of a person as necessary, other researchers have proposed an extension of it, which requires interaction with objects and people in the real world.</p>
            <p>Therefore, in addition to the above, a robot will also need:</p>
            <ul>
                <li><b>Computer vision</b> and <b>speech recognition</b> - to perceive sensory information,</li>
                <li><b>Robotics</b> - to manipulate objects and move through the world.</li>
            </ul>
            <p>While researchers have devoted their efforts towards studying the underlying principles of intelligence, rather than passing the Turing test, the above may still serve as motivation as to why these six disciplines comprise most of AI<sup>[<a href="#aimodernapproach">2</a>]</sup>.</p>
            <p>As we can't mathematically define each aspect of human cognition, the prevailing approach to AI is that of the rational-agent: an entity that acts such that it achieves the best possible outcome given a set of circumstances.</p>
            <p>This aligns well with the skills needed for the Turing test: knowledge representation and reasoning enable agents to take optimal decisions, learning improves the ability to generate effective behaviour even in the face of new circumstances, etc.</p>
            <p>This paradigm became so widely spread that is known under the name of <b>standard model</b>. However, this model does have the limitation of needing to be supplied a fully specified objective.</p>
            <p>This leads to erroneous situations: a self-driving car with the goal of keeping its passengers as safe as possible might never leave the garage, an AI tasked with winning a game might try to do so by any means possible, etc.</p>
            <p>Therefore, it must be added to the standard model the dimension of <b>value alignment</b>: a tradeoff between progress towards the goal and the values of humans and society at large.</p>

        <h2>Historical milestones that influence today's AI</h2>
            <p>A list of important moments in history that influenced the trajectory of today's AI is the following:</p>
            <ul>
                <li><span class="par-title">Neural Networks (1986 - present)</span><br>In the mid-1980s, different groups reinvented the <b>back-propagation</b> learning algorithms that was previously developed in the early 1960s, with Geoffrey Hinton as a leading figure in the resurgence of neural networks.<br>This <b>connectionist</b> approach was seen as direct competition to older <b>symbolic</b> models, having the advantage of forming internal concepts in a more fluid and imprecise way, better suited to the complexity of the real world.</li>

                <li><span class="par-title">Probabilistic reasoning and machine learning (1987 - present)</span><br>The limitations of expert systems (AI programs for decision-making given a domain-specific knowledge base), such as not performing well in the face of uncertainty, led to a new approach to AI: incorporating probability and statistics rather than Boolean logic and machine learning instead of hand-coding.<br>This enabled scientists to build on already existing mathematical research. For example, progress in the field of speech recognition was made through <b>hidden Markov models</b> - statistical models particularly useful for representing sequential data.<br>An influential work in this period was <b>Judea Pearl's</b>, <em>Probabilistic Reasoning in Intelligent Systems</em>. His development of <b>Bayesian networks</b> established a rigorous and efficient framework for representing uncertain knowledge, as well as practical algorithms for probabilistic reasoning.<br>Another major contribution was that of <b>Rich Sutton</b> connecting reinforcement learning to the theory of Markov decision processes, which found applications in robotics and industrial process control.<br>A consequence of AI incorporating statistical modeling, optimization techniques and machine learning resulted in reuniting with subfields such as computer vision, robotics, speech recognition, multi-agent system, natural language processing, which had previously become somewhat separate from core AI. This resulted in significant benefits both in terms of applications and in a better theoretical understanding of the core problems of AI.</li>

                <li><span class="par-title">Big data (2001 - present)</span><br>Advances in computing power and the World Wide Web have facilitated the creation of very large data sets containing billions of words, images, speech and video, as well as domain-specific information such as genomic data, vehicle tracking data, clickstream and social network data, etc.<br>This phenomenon is commonly referred to as <b>big data</b> and has led to the development of learning algorithms specially designed to take advantage of such large data sets.<br>Most often, the examples in these data sets are <b>unlabeled</b>. However, for example, suitable learning algorithms could achieve an accuracy of over 96% in identifying which sense was intended in a sentence. Furthermore, it was argued that stronger improvements could be attained by increasing the data size by two or three orders of magnitude, rather than tweaking the learning algorithm.<br>Another similar example could be observed in computer vision tasks such as filling in holes in photographs by blending in pixels from similar images. It was concluded that this technique worked poorly with a database of only thousands of images, but produces higher quality results with millions of images.<br>Soon after, the availability of <b>ImageNet database</b> comprising of millions of images, greatly accelerate progress in the field of computer vision. Furthermore, the availability of big data and the inclusion of machine learning into AI gave the field a commercial attractiveness it previously lost.</li>

                <li><span class="par-title">Deep Learning (2011 - present)</span><br><b>Deep learning</b> is a subfield of machine learning that uses multiple layers of simple, adjustable computing elements - <b>neural networks</b>.<br>Experiments with neural networks were done in the previous decades and in 1990, the model of <b>convolutional neural networks</b> was successful in recognizing written digits. However, it wasn't until 2011 that deep learning became widely spread, especially in speech and visual object recognition.<br>In the 2012 competition launched by ImageNet, which required classifying images into a thousand categories, a deep learning system created by Geoffrey Hinton's group demonstrated a dramatic improvement over the previous existing systems.<br>Since then, deep learning systems have surpassed human performance on select vision tasks (while lagging in others). Improvements have also been noticed in speech recognition, machine translation, medical diagnosis, and game playing. This has resulted in bringing AI into the public's attention unlike any other point in history.<br>The availability of large amounts of training data, along with advances in computation power have also played a role in developing deep learning applications - a standard CPU can do \(10^9\) or \(10^{10}\) operations per second, while a deep learning algorithm running on specialized hardware (GPU, TPU, FPGA) might consume between \(10^{14}\) or \(10^{17}\) operations.</li>
            </ul>

        <h2>What can AI achieve today?</h2>
            <p>Below are some examples:</p>
            <ul>
                <li><span class="par-title">Robotic vehicles:</span> In 2019, Waymo test vehicles passed the threshold of 10 million miles driven on public roads without a significant accident, with the human driving taking over control only once every 6000 miles. Since 2016, autonomous fixed-wing drones have been providing cross-country blood deliveries in Rwanda.</li>

                <li><span class="par-title">Autonomous planning and scheduling:</span> NASA's Remote Agent program became the first on-board autonomous planning program to control the scheduling of operation for a spacecraft. It was able to generate plans for high-level goals sent from the ground and then monitor their execution by detecting, diagnosing and recovering from problems as they occurred.<br>The <em>Europa</em> planning toolkit is used for daily operations of NASA's Mars rovers and the <em>SEXTANT</em> system enable autonomous navigation in deep space, surpassing the capabilities of the global GPS system.<br>Furthermore, autonomous planning can be observed in the daily lives of regular people: apps capable of plotting optimal routes and take into account current and predicted future conditions such as Uber and Google Maps, provide services for millions of users.</li>

                <li><span class="par-title">Machine Translation:</span> online machine translation systems render hundreds of billions of words per day for hundreds of millions of users. While improvement in their capabilities can be made, they can provide users with adequate results and in the case of closely related languages with a large volume of training data (such as French and English), translation within a narrow domain can approach human levels of performance.</li>

                <li><span class="par-title">Speech Recognition:</span> In 2017, Microsoft showed that its Conversational Speech Recognition System had reached a word error rate of 5.1% in transcribing telephone conversations, matching human performance. This proves useful as about a third of computer interaction worldwide is now done by voice rather than keyboard.<br>Other relevant services: Skype offers real-time speech to speech translation, virtual assistants offered by various companies can answer questions, control smart home devices, set reminders, provide directions, with some even being able to make calls to businesses to book appointments on behalf of the users.</li>

                <li><span class="par-title">Recommendations:</span> Companies such as Amazon, Facebook, Netflix, Spotify, YouTube etc. make use of machine learning to recommend content to users based on their previous experiences and that of similar users.<br>Additionally, spam filtering can also be considered a form of recommendations, with current AI techniques being able to filter out over 99.9% of spam. Email services are also able to recommend potential recipients or possible response texts.</li>

                <li><span class="par-title">Image understanding:</span> Encouraged by the excellent results in object recognition tasks, computer vision researchers have undertaken the more difficult problem of image captioning - generating descriptions depending on the context of an image. However, current systems are still facing significant challenges in this area.</li>

                <li><span class="par-title">Medicine:</span> AI algorithms are able to equal or exceed expert doctors at diagnosing a plethora of conditions, especially when the diagnosis is based on images. Examples include: Alzheimer's disease, metastatic cancer, ophthalmic and skin disease.<br>One current direction of medical AI research is facilitating partnerships between humans and machines. For example, the LYNA system achieves 99.6% overall accuracy in diagnostic metastatic breast cancer, which surpasses the performance of an unaided human expert.<br>Widespread adoption of such techniques are limited by the need to demonstrate improvement in clinical outcomes and ensure transparency, data privacy and lack of bias, rather than by diagnostic accuracy.</li>

                <li><span class="par-title">Climate science:</span> In 2018, a team of scientists won the Gordon Bell Prize for developing a deep learning model that was able to leverage existing, but hard to go through data into discovering detailed information about extreme weather events. They used a supercomputer with a specialized GPU hardware in order to exceed \(10^{18}\) operations per second, being the first machine learning program to do so.</li>
            </ul>

            <div class="line"></div>
            <!-- References -->
            <p id="denning2000computer">[1]: P. J. Denning, "<i>Computer science: The discipline</i>"</p>
            <p id="aimodernapproach">[2]: P. Norvig, S. Russel "<i>Artificial intelligence: A modern approach</i>"</p>
        </div>
    </body>
</html>