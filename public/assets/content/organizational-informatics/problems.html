<html class="base">
<div class="papyrus-container">
    <div class="content">
        <br>
        <h1 class="text-6xl">&nbsp;&nbsp;&nbsp;&nbsp;Problems</h1>
        <hr style="height:2px;border-width:0;color:black;background-color:black">
        <p>&nbsp;&nbsp;&nbsp;&nbsp;Some important problems/experiments:</p>
        <br>
        <dl>
            <dt>
                <b><h1 class="text-2xl">Minnesota Experiments</h1></b>
            </dt>
            <p>&nbsp;&nbsp;&nbsp;&nbsp;The use of computer based information-decision systems to support decision making in organizations has increased significantly in the last decade. Very little effort has been devoted, however, to determine what relationships exist between the structure of information presented for decision making and the ensuing effectiveness of the decision. </p><br><p>&nbsp;&nbsp;&nbsp;&nbsp;The <ins>Minnesota Experiments</ins>, which were conducted to examine the significance of various information system characteristics on decision activity. Several research programs administered in the period 1970â€“1975 are discussed in this paper. By varying the manner in which information was provided to participants in each experiment, the impact of various information system characteristics and individual differences on decision effectiveness was investigated. Analysis of the results shows that, in many cases, the decisions/decision-making process of the participants was affected by the information system structure and/or attributes of individual decision makers.</p><p>&nbsp;&nbsp;&nbsp;&nbsp;The results suggest guidelines for the designers of information systems and fruitful avenues for continued research.</p>
            <hr style="height:2px;border-width:0;color:black;background-color:black">

            <dt>
                <b><h1 class="text-2xl">Online controlled experiments at large scale</h1></b>
            </dt>
            <p style="margin-bottom:7px;">&nbsp;&nbsp;&nbsp;&nbsp;Web-facing companies, including Amazon, eBay, Etsy, Facebook, Google, Groupon, Intuit, LinkedIn, Microsoft, Netflix, Shop Direct, StumbleUpon, Yahoo, and Zynga use online controlled experiments to guide product development and accelerate innovation.</p>

            <p style="margin-bottom:7px;">&nbsp;&nbsp;&nbsp;&nbsp;At <ins>Microsoft's Bing</ins>, the use of controlled experiments has grown exponentially over time, with over 200 concurrent experiments now running on any given day. Running experiments at large scale requires addressing multiple challenges in three areas: <em>cultural/organizational, engineering,</em> and <em>trustworthiness</em>.</p>

            <p style="margin-bottom:7px;">&nbsp;&nbsp;&nbsp;&nbsp;On the <em>cultural</em> and <em>organizational</em> front, the larger organization needs to learn the reasons for running controlled experiments and the tradeoffs between controlled experiments and other methods of evaluating ideas. We discuss why negative experiments, which degrade the user experience short term, should be run, given the learning value and long-term benefits.</p>

            <p style="margin-bottom:7px;">&nbsp;&nbsp;&nbsp;&nbsp;On the <em>engineering</em> side, we architected a highly scalable system, able to handle data at massive scale: hundreds of concurrent experiments, each containing millions of users. Classical testing and debugging techniques no longer apply when there are billions of live variants of the site, so alerts are used to identify issues rather than relying on heavy up-front testing.</p>

            <p style="margin-bottom:7px;">&nbsp;&nbsp;&nbsp;&nbsp;On the <em>trustworthiness</em> front, we have a high occurrence of false positives that we address, and we alert experimenters to statistical interactions between experiments.</p>

            <p style="margin-bottom:7px;">&nbsp;&nbsp;&nbsp;&nbsp;The Bing Experimentation System is credited with having accelerated innovation and increased annual revenues by hundreds of millions of dollars, by allowing us to find and focus on key ideas evaluated through thousands of controlled experiments. A <b>1%</b> improvement to revenue equals more than <b>$10M annually</b> in the US, yet many ideas impact key metrics by 1% and are not well estimated a-priori. The system has also identified many negative features that we avoided deploying, despite key stakeholders' early excitement, saving us similar large amounts.</p>
        </dl>
    </div>
</div>
</html>